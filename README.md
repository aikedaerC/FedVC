We introduce a novel concept termed **Virtual Clients** within the realm of Federated Learning (FL). This innovative approach offers a fresh perspective to address the pervasive challenge of imbalanced label distribution in distributed training scenarios. By analyzing the traditional training processes of both centralized and distributed frameworks, we identified key differences that inspired us to harness the strengths of centralized training to approximate a balanced data distribution in a federated setting. Our proposed training strategy, **FedVC (Federated Virtual Clients)**, leverages this insight to mitigate label imbalance while preserving the privacy and decentralization principles of FL. We believe FedVC holds significant potential for a wide range of downstream applications, including federated autonomous driving, federated image recognition, federated object detection, and so on. This approach not only enhances model performance in heterogeneous data environments but also broadens the applicability of federated learning to more complex and real-world tasks.


We are in an early-release beta. Expect some adventures and rough edges.


